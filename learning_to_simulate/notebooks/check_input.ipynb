{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breaking Down Input Function: `input_fn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook helps us to understand how the data was uploaded to create input_fn callback."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, import the nedeed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../')\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import functools\n",
    "import json\n",
    "from learning_to_simulate import reading_utils\n",
    "from learning_to_simulate import train\n",
    "tf.compat.v1.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function to read metada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _read_metadata(data_path):\n",
    "  with open(os.path.join(data_path, 'metadata.json'), 'rt') as fp:\n",
    "    return json.loads(fp.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the tfrecord and json with the metada.\n",
    "\n",
    "After this cell, the dataset contains tuples `(context, features)`\n",
    "```\n",
    "    context['particle_type'] => tf size: [n_particles]\n",
    "    features['position']     => tf: [steps,n_particles, positions]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "particle type:  (678,)\n",
      "position:  (1001, 678, 2)\n",
      "particle type:  (355,)\n",
      "position:  (1001, 355, 2)\n"
     ]
    }
   ],
   "source": [
    "info_dir = \"/home/zoso/Documents/deepmind-research/information\"\n",
    "data_path = os.path.join(info_dir,'datasets/WaterDropSample/')\n",
    "\n",
    "metadata = _read_metadata(data_path)\n",
    "\n",
    "\n",
    "# Create a tf.data.Dataset from the TFRecord.\n",
    "ds = tf.data.TFRecordDataset([os.path.join(data_path, 'train.tfrecord')])\n",
    "ds = ds.map(functools.partial(reading_utils.parse_serialized_simulation_example, metadata=metadata))\n",
    "\n",
    "for (context, features) in ds.take(2):\n",
    "    print(\"particle type: \",context['particle_type'].shape)\n",
    "    print(\"position: \", features['position'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `mode: one_step`\n",
    "\n",
    "Executing the next set leads us to a ds which contains `element` \n",
    "```\n",
    "element['particle_type'] => tf : [n_particles]\n",
    "element['position']      => rf : [7, n_particles, positions]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "particle type:  (678,)\n",
      "position:  (7, 678, 2)\n",
      "-------------------\n"
     ]
    }
   ],
   "source": [
    "ds1 = ds\n",
    "\n",
    "# So we can calculate the last 5 velocities.\n",
    "INPUT_SEQUENCE_LENGTH = 6\n",
    "batch_size = 2\n",
    "\n",
    "# Splits an entire trajectory into chunks of 7 steps.\n",
    "# Previous 5 velocities, current velocity and target.\n",
    "# It is like a batch of 7 position steps\n",
    "split_with_window = functools.partial(\n",
    "    reading_utils.split_trajectory,\n",
    "    window_length=INPUT_SEQUENCE_LENGTH + 1)\n",
    "ds1 = ds1.flat_map(split_with_window)\n",
    "\n",
    "for elem in ds1.take(1):\n",
    "    print(\"particle type: \", elem['particle_type'].shape)\n",
    "    print(\"position: \", elem['position'].shape)\n",
    "print(\"-------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Executing the next set leads us to a ds which contains tuples `(features,labels)` \n",
    "```\n",
    "features['particle_type']           => tf: [n_particles]\n",
    "features['position']                => tf: [n_particles,6,positions]\n",
    "features['n_particles_per_example'] => tf: [1] value: [n_particles]\n",
    "labels                              => tf: [n_particles]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "particle type:  (678,)\n",
      "position:  (678, 6, 2)\n",
      "n_particles_per_example:  tf.Tensor([678], shape=(1,), dtype=int32)\n",
      "labels:  (678, 2)\n",
      "-------------------\n"
     ]
    }
   ],
   "source": [
    "ds1 = ds1.map(train.prepare_inputs)\n",
    "for (features, labels) in ds1.take(1):\n",
    "    print(\"particle type: \",features['particle_type'].shape)\n",
    "    print(\"position: \", features['position'].shape)\n",
    "    print(\"n_particles_per_example: \",features['n_particles_per_example'])\n",
    "    print(\"labels: \",labels.shape) # the target position\n",
    "    \n",
    "print(\"-------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Executing the next set leads us to a ds which contains tuples `(features,labels)` \n",
    "```\n",
    "features['particle_type']          => tf: [batch_size*n_particles]\n",
    "features['position']               => tf: [batch_size*n_particles,6,positions]\n",
    "features['n_particle_per_example'] => tf: [1] value: batch_size * [n_particles] \n",
    "labels                             => tf: [batch_size*n_particles,positions]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <function _yield_value at 0x7f448dbe6ea0> appears to be a generator function. It will not be converted by AutoGraph.\n",
      "WARNING: Entity <function _yield_value at 0x7f448dbe6ea0> appears to be a generator function. It will not be converted by AutoGraph.\n",
      "particle type:  (1356,)\n",
      "position:  (1356, 6, 2)\n",
      "n_particles_per_example:  tf.Tensor([678 678], shape=(2,), dtype=int32)\n",
      "labels:  (1356, 2)\n"
     ]
    }
   ],
   "source": [
    "ds2 = train.batch_concat(ds1, batch_size)\n",
    "for features, labels in ds2.take(1):\n",
    "    print(\"particle type: \",features['particle_type'].shape)\n",
    "    print(\"position: \", features['position'].shape)\n",
    "    print(\"n_particles_per_example: \",features['n_particles_per_example'])\n",
    "    print(\"labels: \",labels.shape) # the target position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `mode: one_step_train`\n",
    "\n",
    "This point must be executed before the last cell, and just allow us to shuffle the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "particle type:  (678,)\n",
      "position:  (678, 6, 2)\n",
      "n_particles_per_example:  tf.Tensor([678], shape=(1,), dtype=int32)\n",
      "features:  (678, 2)\n"
     ]
    }
   ],
   "source": [
    "ds3 = ds1.repeat()\n",
    "ds3 = ds3.shuffle(512)\n",
    "\n",
    "for (context, features) in ds3.take(1):\n",
    "    print(\"particle type: \",context['particle_type'].shape)\n",
    "    print(\"position: \", context['position'].shape)\n",
    "    print(\"n_particles_per_example: \",context['n_particles_per_example'])\n",
    "    print(\"features: \",features.shape) # the target position\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Executing the next set leads us to a ds which contains tuples `(features,labels)` \n",
    "```\n",
    "features['particle_type']          => tf: [batch_size*n_particles]\n",
    "features['position']               => tf: [batch_size*n_particles,6,positions]\n",
    "features['n_particle_per_example'] => tf: [1] value: batch_size * [n_particles] \n",
    "labels                             => tf: [batch_size*n_particles,positions]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "particle type:  (1356,)\n",
      "position:  (1356, 6, 2)\n",
      "n_particles_per_example:  tf.Tensor([678 678], shape=(2,), dtype=int32)\n",
      "labels:  (1356, 2)\n"
     ]
    }
   ],
   "source": [
    "ds3 = train.batch_concat(ds3, batch_size)\n",
    "for features, labels in ds3.take(1):\n",
    "    print(\"particle type: \",features['particle_type'].shape)\n",
    "    print(\"position: \", features['position'].shape)\n",
    "    print(\"n_particles_per_example: \",features['n_particles_per_example'])\n",
    "    print(\"labels: \",labels.shape) # the target position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `mode: rollout`\n",
    "\n",
    "Executing the next set leads us to a ds which contains tuples `(features,labels)` \n",
    "```\n",
    "features['particle_type']          => tf: [n_particles]\n",
    "features['position']               => tf: [n_particles,steps,positions]\n",
    "features['key']                    => tf: [1] value: id_example\n",
    "features['n_particle_per_example'] => tf: [1] value: [n_particles]\n",
    "features['is_trajectory']          => tf: [1] value: True or False\n",
    "labels                             => tf: [n_particles, positions]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "particle_type:  (678,)\n",
      "position:  (678, 1000, 2)\n",
      "key:  tf.Tensor(0, shape=(), dtype=int64)\n",
      "n_particles_per_example:  tf.Tensor([678], shape=(1,), dtype=int32)\n",
      "is_trajectory:  tf.Tensor([ True], shape=(1,), dtype=bool)\n",
      "labels:  (678, 2)\n",
      "-------------\n",
      "particle_type:  (355,)\n",
      "position:  (355, 1000, 2)\n",
      "key:  tf.Tensor(1, shape=(), dtype=int64)\n",
      "n_particles_per_example:  tf.Tensor([355], shape=(1,), dtype=int32)\n",
      "is_trajectory:  tf.Tensor([ True], shape=(1,), dtype=bool)\n",
      "labels:  (355, 2)\n",
      "-------------\n"
     ]
    }
   ],
   "source": [
    "ds4 = ds.map(train.prepare_rollout_inputs)\n",
    "for features, labels in ds4:\n",
    "    print(\"particle_type: \", features['particle_type'].shape)\n",
    "    print(\"position: \", features['position'].shape)\n",
    "    print(\"key: \", features['key'])\n",
    "    print(\"n_particles_per_example: \",features['n_particles_per_example'] )\n",
    "    print(\"is_trajectory: \", features[\"is_trajectory\"])\n",
    "    print(\"labels: \", labels.shape)\n",
    "    print(\"-------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `main function`\n",
    "Here we test the main function which generates the input_fn function calleable. You need to pass the respectivo `mode` and `split`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "particle type:  (1356,)\n",
      "position:  (1356, 6, 2)\n",
      "n_particles_per_example:  tf.Tensor([678 678], shape=(2,), dtype=int32)\n",
      "labels:  (1356, 2)\n"
     ]
    }
   ],
   "source": [
    "info_dir = \"/home/zoso/Documents/deepmind-research/information\"\n",
    "data_path = os.path.join(info_dir,'datasets/WaterDropSample/')\n",
    "#batch_size = 1\n",
    "#mode = 'rollout'\n",
    "batch_size = 2\n",
    "mode = 'one_step_train'\n",
    "\n",
    "input_fn = train.get_input_fn(data_path, batch_size,\n",
    "                                mode=mode, split='train')\n",
    "\n",
    "dataset = input_fn()\n",
    "\n",
    "if 'one_step' in mode:\n",
    "\n",
    "    for (features, labels) in dataset.take(1):\n",
    "        print(\"particle type: \",features['particle_type'].shape)\n",
    "        print(\"position: \", features['position'].shape)\n",
    "        print(\"n_particles_per_example: \",features['n_particles_per_example'])\n",
    "        print(\"labels: \",labels.shape) # the target position\n",
    "elif mode == 'rollout' and batch_size == 1:\n",
    "    for features, labels in dataset.take(1):\n",
    "        print(\"particle_type: \", features['particle_type'].shape)\n",
    "        print(\"position: \", features['position'].shape)\n",
    "        print(\"key: \", features['key'])\n",
    "        print(\"n_particles_per_example: \",features['n_particles_per_example'] )\n",
    "        print(\"is_trajectory: \", features[\"is_trajectory\"])\n",
    "        print(\"labels: \", labels.shape)\n",
    "        print(\"-------------\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit ('learn-simulate': conda)",
   "language": "python",
   "name": "python36964bitlearnsimulateconda90aa6482bbb549ebbaef506b2225e634"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
